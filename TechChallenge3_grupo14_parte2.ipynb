{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FIAP Pós-Tech - IA para Devs**"
      ],
      "metadata": {
        "id": "pVV1jRO5rtAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grupo 14 (1IADT) - Alunos"
      ],
      "metadata": {
        "id": "CtQvdCkCrvS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Marcelo Henriques da Fonseca - RM353865\n",
        "* Marcos Lopes da Silva Junior - RM353763\n",
        "* Ricardo Báfica Pontes - RM353866"
      ],
      "metadata": {
        "id": "odh4OwazrzBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **O que é Fine-Tuning?**"
      ],
      "metadata": {
        "id": "ZimWAXtvBKBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning é uma técnica poderosa que podemos utilizar para treinar modelos de Inteligência Artificial para propósitos específicos.\n",
        "\n",
        "Ele envolve selecionar e ajustar um modelo pré-treinado (como GPT, Llama, BERT e outros) com um conjunto de dados específico para melhorar seu desempenho em alguma tarefa.\n",
        "\n",
        "Neste projeto, vamos utilizar os dados de produtos da Amazon (tratados e divididos no Notebook Parte 1) para realizar o fine-tuning em um modelo GPT a fim de \"ensina-lo\" a falar sobre essas mercadorias."
      ],
      "metadata": {
        "id": "g5qrSLFiBPDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto**"
      ],
      "metadata": {
        "id": "dLqJWG9or4gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir daqui, vamos utilizar as partes que dividimos do JSON e fizemos o upload para o Google Drive para fazer o fine-tuning de um modelo GPT-4o mini."
      ],
      "metadata": {
        "id": "Stz7p9luAk0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C60YJyJxuQ_i",
        "outputId": "7edcc282-5486-4f49-81af-0fee039e6337"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instalando a biblioteca do OpenAI\n",
        "!pip --quiet install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytPTuakHuY2w",
        "outputId": "4bf504c8-52ca-43e0-afd7-8bf444e2edf8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/375.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importando as bibliotecas necessárias\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "6idmcUUIud-o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo usamos o \"userdata.get\" para recuperar a chave de API da OpenAI que foi definida nos \"Secrets\" no próprio Google Colab."
      ],
      "metadata": {
        "id": "stc1ZDsqBBQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criando o client, com a chave de acesso para a API\n",
        "client = OpenAI(\n",
        "  api_key=userdata.get('OPENAI_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "rw25WV0e7Qlo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste bloco, definiremos uma série de perguntas que serão respondidas pelo modelo base GPT-4o mini, a fim de termos uma base de comparação com o modelo fine-tuned."
      ],
      "metadata": {
        "id": "hTbI1X5uBYKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir as mensagens para enviar\n",
        "messages = [\n",
        "    \"What Buffalo News said about 'The Birds of America: The Bien Chromolithographic Edition'?\"\n",
        "]"
      ],
      "metadata": {
        "id": "Zl9MMIZN8cWE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acesso inicial, sem fine-tunig, para comparação futura\n",
        "\n",
        "# Iterar sobre as mensagens e obter as respostas\n",
        "for i, message_content in enumerate(messages):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that informs customers about the contents of a store's products.\"},\n",
        "            {\"role\": \"user\", \"content\": message_content}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "    print(f\"Response {i+1}:\\n{textwrap.fill(response, 80)}\\n\")"
      ],
      "metadata": {
        "id": "ljf7rmCx4jT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e18c78-f021-4bb6-c652-b2bc0046c45f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1:\n",
            "I don't have access to specific articles or content from the Buffalo News or any\n",
            "other publication. However, \"The Birds of America: The Bien Chromolithographic\n",
            "Edition\" is a well-known work that features stunning illustrations of birds by\n",
            "John James Audubon. The Bien edition, published in the 19th century, is notable\n",
            "for its high-quality chromolithographic reproductions of Audubon's original\n",
            "paintings. If you're looking for a review or specific commentary from the\n",
            "Buffalo News, I recommend checking their archives or website for the most\n",
            "accurate and detailed information.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora iremos converter o nosso arquivo JSON do Google Drive para um arquivo JSONL que é necessário para realizar o fine-tuning pela OpenAI. Também ajustaremos o conteúdo do JSON para um formato compatível para o treinamento.\n",
        "\n",
        "Ao final, salvaremos o novo arquivo de novo no Google Drive."
      ],
      "metadata": {
        "id": "mjpNpmvdFNOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Caminho do arquivo JSON original\n",
        "arquivo_json = '/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_3/trn_100k_1.json'\n",
        "# Caminho para o arquivo JSONL que será gerado\n",
        "arquivo_jsonl = '/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_3/trn_100k_1.jsonl'\n",
        "\n",
        "dados = []\n",
        "\n",
        "# Carregar o arquivo JSON\n",
        "with open(arquivo_json, 'r') as f:\n",
        "    for line in f:\n",
        "        dados.append(json.loads(line))\n",
        "\n",
        "# Verificar se os dados estão em formato de lista\n",
        "if isinstance(dados, list):\n",
        "    # Abrir o arquivo JSONL para escrita\n",
        "    with open(arquivo_jsonl, 'w') as outfile:\n",
        "        for item in dados:\n",
        "            # Ajustar para o formato compatível com a OpenAI\n",
        "            messages = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that informs customers about the contents of a store's products.\"},\n",
        "                    {\"role\": \"user\", \"content\": item['title']},\n",
        "                    {\"role\": \"assistant\", \"content\": item['content']}\n",
        "                ]\n",
        "            }\n",
        "            json.dump(messages, outfile)\n",
        "            outfile.write('\\n')\n",
        "\n",
        "print(\"Arquivo JSON convertido para JSONL com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kappTip7F-7v",
        "outputId": "e4c44a51-7e2b-4fb6-be6e-6eae8d660d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo JSON convertido para JSONL com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse bloco iremos \"subir\" o nosso arquivo de treinamento para o ambiente OpenAI."
      ],
      "metadata": {
        "id": "ei_cD7gcGXtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.files.create(\n",
        "  file=open(\"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_3/trn_100k_1.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7mcvrzzSfOd",
        "outputId": "3a41b378-a2b2-4371-a46b-0a09678ff11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-aH7jLrHN6m2i1rNII65wx59t', bytes=136677541, created_at=1726346872, filename='trn_100k_1.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este projeto de Fine-Tuning decidimos utilizar o modelo GPT-4o mini da OpenAI. Ele muito bom e econômico, que se beneficia da arquitetura do GPT-4 em um modelo mais compacto.\n",
        "\n",
        "Sendo uma versão mais leve, ele requer menos recursos computacionais e seu treinamento é mais rápido e barato, sendo uma ótimo opção para os propósitos didáticos deste projeto."
      ],
      "metadata": {
        "id": "tag1nAXTHF2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A OpenAI permite que selecionemos alguns hiperparâmetros para ajustar o nosso fine-tuning.\n",
        "\n",
        "**Epoch:** refere-se a quantidade de vezes que o modelo processa o conjunto de dados, quantas vezes ele \"passa\" pelas informações de treinamento.\n",
        "\n",
        "Um número maior de epochs pode fazer com que o modelo aprenda melhor dados, mas um número muito grande pode causar overfitting (fazendo com que ele apenas repita os dados que foram passados). Pelo contrário, um número insuficiente de epochs pode fazer com que o modelo não aprenda tão bem.\n",
        "\n",
        "**Learning Rate Multiplier:** refere-se a taxa de aprendizado, que define a rapidez com que o modelo ajusta seus pesos durante o processo de treinamento.\n",
        "\n",
        "Um Learning Rate Multiplier mais alto pode acelerar o treinamento, mas também pode fazer com que o modelo não converja em soluções ótimos, o tornando mais instável. Pelo contrário, um Learning Rate Multiplier mais baixo pode ajudar o modelo a fazer ajustes finos, mas também pode tornar o treinamento muito lento e fazer com que ele fique preso em mínimos locais, prejudicando a capacidade de aprendizado.\n",
        "\n",
        "**Batch Size:** refere-se ao tamanho do lote de dados processado antes do modelo atualizar seus pesos.\n",
        "\n",
        "Um Batch Size maior pode ser mais estável e resultar em um convergência mais suave, mas requer mais memória e maior tempo de treinamento. Pelo contrário, um Batch Size menor pode fazer com que o treinamento seja mais rápido, mas pode resultar em um modelo mais instável."
      ],
      "metadata": {
        "id": "4IbR0WkeEjiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após diversos testes com outros conjutos de dados menores, chegamos nos hiperparâmetros abaixo para o nosso fine-tuning final.\n",
        "\n",
        "Um número de epochs de 2 pode parecer baixo, mas se mostrou adequado julgando os propósitos acadêmicos desse projeto e o tamanho dos dados a serem treinados. Decidimos evitar um número muito grande de epochs para reduzir o tempo de fine-tuning do modelo.\n",
        "\n",
        "Neste caso, um learning rate multiplier um pouco mais alto de 1.8, nos ajudou a chegar em um bom modelo com um número de epochs reduzido.\n",
        "\n",
        "O batch size escolhido de 66 é moderado, e não vimos necessidade de altera-lo neste caso."
      ],
      "metadata": {
        "id": "kPj5EU-yIr4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2\n",
        "learning_rate_multiplier = 1.8\n",
        "batch_size = 66\n",
        "\n",
        "parametros={\"n_epochs\": n_epochs, \"learning_rate_multiplier\": learning_rate_multiplier, \"batch_size\": batch_size}"
      ],
      "metadata": {
        "id": "Ae_ZUaQEI01a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O seguinte bloco cria um job de fine-tuning usando o modelo indicado (GPT-4o mini) e o nosso arquivo de treinamento."
      ],
      "metadata": {
        "id": "iUYziYJcGoOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=\"file-aH7jLrHN6m2i1rNII65wx59t\",\n",
        "  model=\"gpt-4o-mini-2024-07-18\",\n",
        "  ### PARÂMETROS\n",
        "  hyperparameters=parametros\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pz6FAvz_qUY",
        "outputId": "383a37e0-c433-49ab-fb18-2e00c7d904b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-4uaOwnmSMwdYaTdeaD1XsYvF', created_at=1726346895, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-JVwSQhJq81swAguZeRqpHGfb', result_files=[], seed=1288320048, status='validating_files', trained_tokens=None, training_file='file-aH7jLrHN6m2i1rNII65wx59t', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os 3 blocos abaixo recuperam algumas informações referentes ao nosso job de fine-tuning."
      ],
      "metadata": {
        "id": "WkTiPMRxHSs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista 10 fine-tuning jobs\n",
        "client.fine_tuning.jobs.list(limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4GZcngUBZcw",
        "outputId": "705866ec-1fdc-41d9-be9a-e01bccab4485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-4uaOwnmSMwdYaTdeaD1XsYvF', created_at=1726346895, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-JVwSQhJq81swAguZeRqpHGfb', result_files=[], seed=1288320048, status='validating_files', trained_tokens=None, training_file='file-aH7jLrHN6m2i1rNII65wx59t', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None), FineTuningJob(id='ftjob-Ksoe0QtExNhQQpgl6UTBpkZG', created_at=1726342333, error=Error(code='invalid_training_file', message='The job failed due to an invalid training file. Invalid file format. Input file file-qi5PJGB680xpxOvNPXxPW9k2 is in the prompt-completion format, but the specified model gpt-4o-mini-2024-07-18 is a chat model and requires chat-formatted data. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for details.', param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-JVwSQhJq81swAguZeRqpHGfb', result_files=[], seed=1740282797, status='failed', trained_tokens=None, training_file='file-qi5PJGB680xpxOvNPXxPW9k2', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)], object='list', has_more=False)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recupera o estado de um fine tune\n",
        "client.fine_tuning.jobs.retrieve(\"ftjob-4uaOwnmSMwdYaTdeaD1XsYvF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSpBc79GCA6r",
        "outputId": "06b2307a-b72a-4ded-e3c1-a9a99bab19d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-4uaOwnmSMwdYaTdeaD1XsYvF', created_at=1726346895, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1, batch_size=66, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-JVwSQhJq81swAguZeRqpHGfb', result_files=[], seed=1288320048, status='validating_files', trained_tokens=None, training_file='file-aH7jLrHN6m2i1rNII65wx59t', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista 10 eventos de um fine-tuning job\n",
        "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-4uaOwnmSMwdYaTdeaD1XsYvF\", limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiMQGsOXT-UJ",
        "outputId": "454a7ef9-7b02-41eb-c25a-cb5ba869dc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-7GCUzmF524kAQizhqADoGs9A', created_at=1726346895, level='info', message='Validating training file: file-aH7jLrHN6m2i1rNII65wx59t', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-mHYXgLM55i3idIB3rNqeqfsQ', created_at=1726346895, level='info', message='Created fine-tuning job: ftjob-4uaOwnmSMwdYaTdeaD1XsYvF', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o modelo fine-tuned, vamos realizar um teste passando algumas perguntas e vendo como ele as responde."
      ],
      "metadata": {
        "id": "D7pjOaJBHf6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterar sobre as mensagens e obter as respostas\n",
        "for i, message_content in enumerate(messages):\n",
        "    completion_test = client.chat.completions.create(\n",
        "        model=\"ft:gpt-4o-mini-2024-07-18:personal::A7XET4xh\", # nosso modelo fine-tuned\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that informs customers about the contents of a store's products.\"},\n",
        "            {\"role\": \"user\", \"content\": message_content}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    response_test = completion_test.choices[0].message.content\n",
        "    print(f\"Response {i+1}:\\n{textwrap.fill(response, 80)}\\n\")\n",
        "    print(f\"Response Fine-Tuned {i+1}:\\n{textwrap.fill(response_test, 80)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLF3iNJGVhTW",
        "outputId": "cd075b14-013c-4383-9181-6cf60f21df0b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1:\n",
            "I don't have access to specific articles or content from the Buffalo News or any\n",
            "other publication. However, \"The Birds of America: The Bien Chromolithographic\n",
            "Edition\" is a well-known work that features stunning illustrations of birds by\n",
            "John James Audubon. The Bien edition, published in the 19th century, is notable\n",
            "for its high-quality chromolithographic reproductions of Audubon's original\n",
            "paintings. If you're looking for a review or specific commentary from the\n",
            "Buffalo News, I recommend checking their archives or website for the most\n",
            "accurate and detailed information.\n",
            "\n",
            "Response Fine-Tuned 1:\n",
            "“An extraordinary work of art.” (Buffalo News)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content no JSON: “Staggeringly beautiful.” (Buffalo News)\n",
        "\n",
        "Comparando as duas respostas, podemos verificar que o modelo fine-tuned aprendeu com o conteúdo do JSON. Apesar da resposta não ser exata, o modelo tem conhecimento de que o Buffalo News fez um comentário positivo a respeito do livro, diferente do modelo base que não tem conhecimento algum sobre a citação do jornal."
      ],
      "metadata": {
        "id": "wqlS26f7IJSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Possível aprimoramento do modelo**"
      ],
      "metadata": {
        "id": "eomw_nUe8IUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para propósitos didáticos e levando em conta os requisitos do projeto, julgamos que um fine-tuning com 100.000 linhas é o suficiente para atingir nosso objetivo.\n",
        "\n",
        "Entretanto, vale ressaltar que seria possível melhorar esse modelo com mais dados e expandir seu leque de conhecimento além das 100.000 que ele aprendeu.\n",
        "\n",
        "Os códigos abaixo são um exemplo de como poderiamos realizar outros fine-tunings em cima do modelo fine-tuned para criar um modelo aprimorado.\n",
        "\n",
        "Se quiséssemos fazer fine-tuning em um modelo para que ele aprendesse todos os mais de 1 milhão de dados do dataset original da Amazon, precisaríamos apenas treinar o modelo várias vezes, sempre iterando sobre o modelo fine-tuned mais recente com um arquivo com mais dados (trn_100k_2, trn_100k_3, trn_100k_4 ...)."
      ],
      "metadata": {
        "id": "3qSCQCOWIicW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.files.create(\n",
        "  file=open(\"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_3/trn_100k_2.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "Cl7r7Bly9CKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dispara job de criação do modelo, com fine-tuning, complementar aos treinamentos anteriores\n",
        "\n",
        "# nome do modelo\n",
        "# aqui podemos passar o nome de um modelo já ajustado anteriormente com fine-tuning\n",
        "# isso iniciará um novo job de fine-tuning usando o modelo anterior como ponto de partida.\n",
        "model_name = \"ft:gpt-4o-mini-2024-07-18:personal::A7XET4xh\"\n",
        "# hiperparâmetros a serem utilizados\n",
        "# a documentação da OpenAI recomenda treinar inicialmente sem especificar nenhum deles,\n",
        "# permitindo a eles escolher um padrão com base no tamanho do conjunto de dados e, em seguida,\n",
        "# ajustar se observarmos o seguinte:\n",
        "# se o modelo não seguir os dados de treinamento tanto quanto esperado, aumente o número de épocas em 1 ou 2\n",
        "#     isso é mais comum para tarefas para as quais existe uma única conclusão ideal (ou um pequeno conjunto de conclusões ideais semelhantes).\n",
        "#     alguns exemplos incluem classificação, extração de entidade ou análise estruturada.\n",
        "#     freqüentemente, essas são tarefas para as quais você pode calcular uma métrica de precisão final em relação a uma resposta de referência.\n",
        "# se o modelo se tornar menos diversificado do que o esperado, diminua o número de épocas em 1 ou 2\n",
        "#     isso é mais comum para tarefas para as quais há uma ampla gama de possíveis boas conclusões\n",
        "n_epochs = 2\n",
        "\n",
        "# se o modelo não parecer convergir, aumente o multiplicador da taxa de aprendizagem\n",
        "learning_rate_multiplier = 1.8\n",
        "\n",
        "batch_size = 66\n",
        "\n",
        "# incluir na funcção abaixo, sfoc\n",
        "hyperparameters={\"n_epochs\": n_epochs, \"learning_rate_multiplier\": learning_rate_multiplier, \"batch_size\": batch_size}\n",
        "\n",
        "###########################################################################\n",
        "# código abaixo comentado para não disparar nova criação inadvertidamente #\n",
        "###########################################################################\n",
        "\n",
        "# # treinamento do modelo\n",
        "# client.fine_tuning.jobs.create(\n",
        "#     training_file=training_file_id,\n",
        "#     model=model_name,\n",
        "#     hyperparameters=hyperparameters\n",
        "# )"
      ],
      "metadata": {
        "id": "Ts4VezsztBcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusão**"
      ],
      "metadata": {
        "id": "Whb02vuXISDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste projeto criamos um modelo GPT-4o mini fine-tuned com dados de produtos da Amazon. Pudemos verificar as nuances no treinamento e observar as diversas de um modelo \"base\" para um modelo \"especializado\".\n",
        "\n",
        "Na prática, o fine-tuning de um modelo de IA é muito útil. Treinar um modelo do zero é extremamente demorado, difícil e consome muitos recursos. O fine-tuning aproveita o conhecimento de um modelo já pré-treinado e o especializa.\n",
        "\n",
        "Ele é muito técnica muito poderosa quando queremos personalizar um modelo para realizar tarefas específicas.\n",
        "\n",
        "Casos de uso podem incluir o uso de fine-tuning para a criação de chatbots, assistentes virtuais, geração de conteúdos, análise de sentimentos, análises médicas e outros."
      ],
      "metadata": {
        "id": "4kifPgUfIYTD"
      }
    }
  ]
}